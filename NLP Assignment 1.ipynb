{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e99d458-fbd0-4135-8515-fb074a1a19b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Explain One-Hot Encoding\n",
    "# 2. Explain Bag of Words\n",
    "# 3. Explain Bag of N-Grams\n",
    "# 4. Explain TF-IDF\n",
    "# 5. What is OOV problem?\n",
    "# 6. What are word embeddings?\n",
    "# 7. Explain Continuous bag of words (CBOW)\n",
    "# 8. Explain SkipGram\n",
    "# 9. Explain Glove Embeddings.\n",
    "\n",
    "\n",
    "# Ans:\n",
    "# 1. One-Hot Encoding is a technique used to represent categorical data as binary vectors. Each category is assigned a unique index, and a\n",
    "# vector of all zeros except for the index corresponding to the category, which is set to one.\n",
    "\n",
    "# 2. Bag of Words is a text representation technique where the occurrence or frequency of words in a document is used to create a numerical\n",
    "# feature vector. It disregards the order and structure of the text.\n",
    "\n",
    "# 3. Bag of N-Grams is an extension of Bag of Words that includes sequences of N consecutive words as features. It captures more contextual\n",
    "# information compared to Bag of Words by considering word sequences.\n",
    "\n",
    "# 4. TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic that reflects the importance of a word in a document \n",
    "# collection. It measures the frequency of a word in a document relative to its frequency across all documents, providing a measure of\n",
    "# how unique and significant a word is to a particular document.\n",
    "\n",
    "# 5. OOV (Out-of-Vocabulary) problem refers to the challenge of encountering words in a text that are not present in the vocabulary or\n",
    "# training data of a language model. It can pose difficulties in understanding and generating accurate predictions for such words.\n",
    "\n",
    "# 6. Word embeddings are dense vector representations of words in a continuous multi-dimensional space. They capture semantic and \n",
    "# syntactic relationships between words, enabling machines to better understand and process natural language.\n",
    "\n",
    "# 7. Continuous Bag of Words (CBOW) is a word embedding model that predicts a target word based on the context words surrounding it. \n",
    "# It learns to associate words with their neighboring words and generates word embeddings based on this context.\n",
    "\n",
    "# 8. SkipGram is a word embedding model that predicts the context words given a target word. It aims to learn the relationships between \n",
    "# a word and its surrounding words, resulting in word embeddings that capture the semantic and syntactic similarities between words.\n",
    "\n",
    "# 9. GloVe (Global Vectors for Word Representation) is a word embedding technique that combines the global co-occurrence statistics of\n",
    "# words in a corpus with matrix factorization methods. It generates word embeddings that capture both local and global word-to-word \n",
    "# relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8facc40-9336-44c2-872e-e774af3f2e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
