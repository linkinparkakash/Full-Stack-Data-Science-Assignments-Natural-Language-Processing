{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c13956-e838-424b-95c2-38c0d5fef6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What are Sequence-to-sequence models?\n",
    "# 2. What are the Problem with Vanilla RNNs?\n",
    "# 3. What is Gradient clipping?\n",
    "# 4. Explain Attention mechanism\n",
    "# 5. Explain Conditional random fields (CRFs)\n",
    "# 6. Explain self-attention\n",
    "# 7. What is Bahdanau Attention?\n",
    "# 8. What is a Language Model?\n",
    "# 9. What is Multi-Head Attention?\n",
    "# 10. What is Bilingual Evaluation Understudy (BLEU)\n",
    "\n",
    "\n",
    "# Ans:\n",
    "# 1. Sequence-to-sequence models are a type of neural network architecture designed to handle input and output sequences of variable \n",
    "# lengths. They consist of an encoder component that processes the input sequence and a decoder component that generates the output sequence,\n",
    "# allowing for tasks such as machine translation, text summarization, and speech synthesis.\n",
    "\n",
    "# 2. Vanilla RNNs suffer from the problem of vanishing or exploding gradients, which occurs when the gradients either diminish or grow\n",
    "# exponentially as they are backpropagated through time. This makes it difficult for the network to effectively learn long-term\n",
    "# dependencies in sequential data.\n",
    "\n",
    "# 3. Gradient clipping is a technique used to prevent exploding gradients during training. It involves rescaling the gradients when\n",
    "# their norm exceeds a predefined threshold. By limiting the magnitude of the gradients, gradient clipping helps stabilize the training\n",
    "# process and prevents numerical instability.\n",
    "\n",
    "# 4. Attention mechanism is a mechanism used in sequence-to-sequence models to selectively focus on different parts of the input sequence \n",
    "# when generating the output sequence. It assigns different weights or attention scores to different elements of the input sequence,\n",
    "# allowing the model to attend to the most relevant information during the decoding process.\n",
    "\n",
    "# 5. Conditional random fields (CRFs) are probabilistic models used for sequence labeling tasks, such as part-of-speech tagging or named \n",
    "# entity recognition. CRFs model the conditional probability of a label sequence given an input sequence, taking into account both the \n",
    "# input features and the dependencies between labels.\n",
    "\n",
    "# 6. Self-attention, also known as intra-attention, is an attention mechanism that computes attention weights within the same sequence.\n",
    "# It allows each element in the sequence to attend to other elements in the sequence, capturing the dependencies and relationships\n",
    "# between different positions in the sequence.\n",
    "\n",
    "# 7. Bahdanau Attention is an attention mechanism introduced in the context of sequence-to-sequence models. It computes attention weights\n",
    "# by learning a compatibility score between the hidden state of the decoder and the hidden states of the encoder, enabling the \n",
    "# decoder to attend to different parts of the input sequence at each decoding step.\n",
    "\n",
    "# 8. A Language Model is a model that assigns probabilities to sequences of words or characters in a language. It captures the statistical\n",
    "# patterns and dependencies in a language, allowing for tasks such as generating text, predicting the next word in a sequence, or \n",
    "# evaluating the likelihood of a given sequence.\n",
    "\n",
    "# 9. Multi-Head Attention is an extension of the attention mechanism that allows the model to attend to different parts of the input\n",
    "# sequence simultaneously and learn different representations. It involves performing attention computations with multiple sets of \n",
    "# learnable projection matrices, enabling the model to capture diverse information and improve performance.\n",
    "\n",
    "# 10. Bilingual Evaluation Understudy (BLEU) is a metric used to evaluate the quality of machine translation output. It compares \n",
    "# the machine-generated translation to one or more reference translations, computing a score based on the overlap of n-grams between \n",
    "# the candidate translation and the reference translations. BLEU is widely used to assess the performance of machine translation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d144f2-9a59-44cf-b776-88951c089d59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
