{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3057289-c465-4c26-82a8-6cf947b92eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What are Vanilla autoencoders\n",
    "# 2. What are Sparse autoencoders\n",
    "# 3. What are Denoising autoencoders\n",
    "# 4. What are Convolutional autoencoders\n",
    "# 5. What are Stacked autoencoders\n",
    "# 6. Explain how to generate sentences using LSTM autoencoders\n",
    "# 7. Explain Extractive summarization\n",
    "# 8. Explain Abstractive summarization\n",
    "# 9. Explain Beam search\n",
    "# 10. Explain Length normalization\n",
    "# 11. Explain Coverage normalization\n",
    "# 12. Explain ROUGE metric evaluation\n",
    "\n",
    "\n",
    "# Ans:\n",
    "# 1. Vanilla autoencoders are basic autoencoder models that consist of an encoder network that compresses the input data into a\n",
    "# lower-dimensional representation and a decoder network that reconstructs the original input from the compressed representation. \n",
    "# They are trained to minimize the reconstruction error, aiming to learn an efficient data representation.\n",
    "\n",
    "# 2. Sparse autoencoders are a type of autoencoders that incorporate sparsity constraints during training. They encourage the activation \n",
    "# of only a small subset of neurons in the hidden layer, leading to more sparse and meaningful representations. Sparse autoencoders can \n",
    "# help in feature selection and learning more compact representations.\n",
    "\n",
    "# 3. Denoising autoencoders are trained to reconstruct clean input data from corrupted or noisy versions of the data. By forcing the\n",
    "# autoencoder to recover the original data from the noisy input, denoising autoencoders learn robust representations that can effectively\n",
    "# filter out noise and enhance the quality of the reconstructed output.\n",
    "\n",
    "# 4. Convolutional autoencoders are autoencoder models that utilize convolutional layers in the encoder and decoder networks.\n",
    "# They are specifically designed for handling data with grid-like structures, such as images. Convolutional autoencoders leverage \n",
    "# the spatial relationship between neighboring pixels to capture hierarchical and spatially invariant features in the input data.\n",
    "\n",
    "# 5. Stacked autoencoders, also known as deep autoencoders, are composed of multiple layers of encoders and decoders. Each layer of the\n",
    "# encoder network learns increasingly abstract representations of the input data, and the corresponding decoder network reconstructs the \n",
    "# original data from each layer's representation. Stacked autoencoders can capture complex patterns and hierarchical structures in the data.\n",
    "\n",
    "# 6. LSTM autoencoders can be used to generate sentences by training the autoencoder to encode and decode sequences of words.\n",
    "# The LSTM (Long Short-Term Memory) units in the autoencoder help capture the dependencies and contextual information in the input\n",
    "# sequence. By feeding a partially observed input sequence to the encoder and decoding from the learned representation, the LSTM \n",
    "# autoencoder can generate coherent and contextually relevant sentences.\n",
    "\n",
    "# 7. Extractive summarization is a technique used to create summaries by selecting and combining important sentences or phrases from the \n",
    "# source text. It involves identifying the most salient information in the original document and extracting those sentences that best\n",
    "# represent the key points or overall meaning.\n",
    "\n",
    "# 8. Abstractive summarization is a technique that aims to generate a summary by understanding the content of the source text and \n",
    "# expressing it in a concise and coherent manner using natural language generation. Unlike extractive summarization, abstractive \n",
    "# summarization can generate new sentences that may not appear verbatim in the source text.\n",
    "\n",
    "# 9. Beam search is a decoding algorithm commonly used in sequence generation tasks. It maintains a beam of top-k most likely sequences \n",
    "# at each decoding step and explores different possibilities by expanding each candidate sequence. Beam search helps to generate diverse\n",
    "# and high-quality sequences by considering multiple candidates and selecting the most promising ones.\n",
    "\n",
    "# 10. Length normalization is a technique used to mitigate the bias towards shorter sequences in sequence generation tasks. It involves\n",
    "# dividing the log-probabilities of generated sequences by their length, compensating for the fact that shorter sequences tend to have \n",
    "# higher probabilities due to the exponential nature of language models.\n",
    "\n",
    "# 11. Coverage normalization is a technique used in abstractive summarization to ensure that important information from the source text \n",
    "# is adequately covered in the generated summary. It maintains a coverage vector that keeps track of which parts of the source text have \n",
    "# been attended to during the decoding process, encouraging the model to distribute attention more evenly and avoid excessive repetition.\n",
    "\n",
    "# 12. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate the quality of summaries by comparing \n",
    "# them to one or more reference summaries. ROUGE metrics compute the overlap of n-grams (such as unigrams, bigrams, or skip-b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a403af41-5834-44d6-98ae-6dce4b066ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
