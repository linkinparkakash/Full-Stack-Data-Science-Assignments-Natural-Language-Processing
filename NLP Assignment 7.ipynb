{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "713668fb-7ae7-4ddf-88a0-9b3ebbdb8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Explain the architecture of BERT\n",
    "# 2. Explain Masked Language Modeling (MLM)\n",
    "# 3. Explain Next Sentence Prediction (NSP)\n",
    "# 4. What is Matthews evaluation?\n",
    "# 5. What is Matthews Correlation Coefficient (MCC)?\n",
    "# 6. Explain Semantic Role Labeling\n",
    "# 7. Why Fine-tuning a BERT model takes less time than pretraining\n",
    "# 8. Recognizing Textual Entailment (RTE)\n",
    "# 9. Explain the decoder stack of GPT models.\n",
    "\n",
    "\n",
    "# Ans:\n",
    "# 1. BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based neural network architecture for pretraining \n",
    "# language models. It consists of a stack of transformer encoder layers. BERT employs a bidirectional approach, where the model processes\n",
    "# the input text in both directions (left-to-right and right-to-left) to capture contextual information effectively. The architecture \n",
    "# incorporates attention mechanisms and self-attention mechanisms, allowing the model to attend to different parts of the input sequence\n",
    "# during training and inference.\n",
    "\n",
    "# 2. Masked Language Modeling (MLM) is a pretraining task used in BERT. In MLM, a certain percentage of input tokens are randomly masked, \n",
    "# and the model is trained to predict the original masked tokens. By learning to fill in the masked tokens, BERT captures the bidirectional \n",
    "# contextual understanding of words and their relationships within the given sentence.\n",
    "\n",
    "# 3. Next Sentence Prediction (NSP) is another pretraining task employed in BERT. NSP involves training the model to predict whether two \n",
    "# sentences in a given pair appear consecutively or not. This task helps BERT understand sentence-level relationships and dependencies,\n",
    "# allowing it to generate coherent and contextually relevant responses.\n",
    "\n",
    "# 4. Matthews evaluation is an evaluation metric used primarily for binary classification tasks. It measures the quality of predictions by\n",
    "# considering true positives, true negatives, false positives, and false negatives in a confusion matrix. The Matthews evaluation\n",
    "# provides an overall assessment of classification performance, considering both accuracy and imbalance in the dataset.\n",
    "\n",
    "# 5. Matthews Correlation Coefficient (MCC) is a single metric derived from the Matthews evaluation. It ranges between -1 and 1, where 1 \n",
    "# represents a perfect prediction, 0 indicates a random prediction, and -1 indicates complete disagreement between predictions and\n",
    "# ground truth. MCC takes into account true positives, true negatives, false positives, and false negatives, making it suitable for \n",
    "# assessing classification models, especially in imbalanced datasets.\n",
    "\n",
    "# 6. Semantic Role Labeling (SRL) is a natural language processing task that involves assigning semantic roles to words or phrases in a\n",
    "# sentence, identifying their relationships with the predicate (usually a verb). SRL aims to understand the predicate-argument structure\n",
    "# and capture the roles played by different entities, such as the agent, patient, or instrument. It helps in understanding the meaning \n",
    "# and deeper semantics of sentences.\n",
    "\n",
    "# 7. Fine-tuning a BERT model takes less time than pretraining because the pretrained BERT model already captures a significant amount \n",
    "# of language knowledge and understanding. During pretraining, BERT learns general language representations using a massive amount of \n",
    "# unlabeled data. Fine-tuning involves training the pretrained BERT model on a task-specific labeled dataset, which requires comparatively \n",
    "# less data and computational resources. Fine-tuning allows the model to adapt to the specific task and dataset by adjusting the parameters\n",
    "# based on the task-specific objective.\n",
    "\n",
    "# 8. Recognizing Textual Entailment (RTE) is a natural language understanding task that involves determining the logical relationship \n",
    "# between a given pair of texts, typically a premise and a hypothesis. The task aims to identify whether the hypothesis can be inferred \n",
    "# or logically entailed from the given premise. RTE helps in evaluating the model's ability to understand and reason about the meaning \n",
    "# and relationships between different texts.\n",
    "\n",
    "# 9. The decoder stack in GPT (Generative Pre-trained Transformer) models consists of multiple layers of transformer decoders. Each \n",
    "# decoder layer attends to the output of the previous decoder layer, allowing the model to capture contextual information and generate \n",
    "# meaningful representations. The decoder stack plays a crucial role in autoregressive language modeling, where the model predicts\n",
    "# the next word based on the previously generated words. By stacking multiple decoder layers, GPT models can learn complex language \n",
    "# patterns and generate coherent and contextually relevant sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd09d676-ad73-498e-a18d-9598b16cc84c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
